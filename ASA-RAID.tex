
\section{RAID}

Los \emph{arrays} RAID (Redundant Array of Independent Disks) son dispositivos virtuales creados como combinación de dos o más dispositivos físicos. El dispositivo virtual resultante puede contener un filesystem. 

Los diferentes modos de combinación de dispositivos, llamados niveles RAID, ofrecen diferentes características de redundancia y performance. Un array RAID con redundancia ofrece protección contra fallos de dispositivos. 

Los dispositivos Software RAID de Linux son creados y manejados por el driver \lstinline{md} y por eso suelen recibir nombres como \lstinline{md0}, \lstinline{md1}, etc.

 
\begin{itemize}
	\item Redundancia para tolerancia a fallos
	\item Mejoramiento de velocidad de acceso
	\item Niveles 
	\item Hardware RAID, fake RAID, Software RAID
	\item Devices
	\item Spare disks, faulty disks
\end{itemize}



\subsection {Niveles RAID}

\begin{description}
	\item [Linear mode] Dos o más dispositivos concatenados. La escritura de datos ocupa los dispositivos en el orden en que son declarados. 
Sin redundancia.
Mejora la performance cuando diferentes usuarios acceden a diferentes secciones del file system, soportadas en diferentes dispositivos.
	\item [RAID-0] Las operaciones son distribuidas (\emph{striped}) entre los dispositivos, alternando circularmente entre ellos. Cada dispositivo se accede en paralelo, mejorando el rendimiento. Sin redundancia. 
	\item [RAID-1]
Dos o más dispositivos replicados (\emph{mirrored}), con cero o más \emph{spares}. 
Con redundancia. Los dispositivos deben ser del mismo tamaño. Si existen \emph{spares}, en caso de falla o salida de servicio de un dispositivo, el sistema reconstruirá automáticamente una réplica de los datos sobre uno de ellos. 
En un RAID-1 de $N$ dispositivos, pueden fallar o quitarse hasta $N-1$ de ellos sin afectar la disponibilidad de los datos. 
Si $N$ es grande, el bus de I/O puede ser un cuello de botella (al contrario que en Hardware RAID-1). El scheduler de Software RAID en Linux asigna las lecturas a aquel dispositivo cuya cabeza lectora está más cerca de la posición buscada. 
	\item [RAID-4] No se usa frecuentemente. Usado sobre tres o más dispositivos. Mantiene información de paridad sobre un dispositivo, y escribe sobre los restantes en la misma forma que RAID-0. El tamaño del array será $(N-1)*S$, donde $S$ es el tamaño del dispositivo de menor capacidad en el array. 
Al fallar un dispositivo, los datos se reconstruirán automáticamente usando la información de paridad. El dispositivo que soporta la paridad se constituye en el cuello de botella del sistema. 


\item [RAID-5]
Utilizado sobre tres o más dispositivos con cero o más \emph{spares}. El tamaño del dispositivo RAID será $(N-1)*S$. La diferencia con RAID-4 es que la información de paridad se distribuye entre los dispositivos, eliminando el cuello de botella de RAID-4 y obteniendo mejor performance en lectura. Al fallar uno de los dispositivos, los datos siguen disponibles. Si existen \emph{spares}, el sistema reconstruirá automáticamente el dispositivo faltante. Si se pierden dos o más dispositivos simultáneamente, o durante una reconstrucción, los datos se pierden definitivamente. RAID-5 sobrevive a la falla de un dispositivo, pero no de dos o más. 
La performance en lectura y escritura mejora con respecto a un solo dispositivo. 

\item [RAID-6]
Usado sobre cuatro o más dispositivos con cero o más \emph{spares}. La diferencia con RAID-5 es que existen dos diferentes bloques de información de paridad, distribuidos entre los dispositivos participantes, mejorando la robustez. El tamaño del dispositivo RAID-6 es $(N-2)*S$. Si fallan uno o dos de los dispositivos, los datos siguen disponibles. Si existen \emph{spares}, el sistema reconstruirá automáticamente los dispositivos faltante. La performance en lectura es similar a RAID-5, pero la de escritura no es tan buena.

\item [RAID-10]
Combinación de RAID-1 y RAID-0 completamente ejecutada por el kernel, más eficiente que aplicar dos niveles de RAID independientemente. Es capaz de aumentar la eficiencia en lectura de acuerdo a la cantidad de dispositivos, en lugar de la cantidad de pares RAID-1, ofreciendo un 95\% del rendimiento de RAID-0 con la misma cantidad de dispositivos. Los \emph{spares} pueden ser compartidos entre todos los pares RAID-1.



\item [FAULTY]

Nivel especial de RAID que sirve para debugging del array por inyección de fallos de lectura y escritura. Sólo permite un dispositivo. Simula fallos a bajo nivel, permitiendo analizar comportamiento en caso de fallos de sector en lugar de fallos de discos.

\end{description}



\subsection{Construcción y uso de un array RAID-1}



Crear particiones en ambos discos, tipo fd (Linux RAID autodetect)
\begin{lstlisting}
fdisk /dev/sdb; fdisk /dev/sdc
\end{lstlisting}

Crear el array
\begin{lstlisting}
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
watch cat /proc/mdstat
\end{lstlisting}

Usar el array
\begin{lstlisting}
mkfs -t ext3 /dev/md0
mkdir /datos
mount -t ext3 /dev/md0 /datos
cp /etc/hosts /datos
ll /datos
\end{lstlisting}

Examinar el array
\begin{lstlisting}
cat /proc/mdstat
cat /proc/partitions
mdadm --examine --brief --scan --config=partitions
mdadm --examine /dev/sdc
mdadm --query --detail /dev/md0
\end{lstlisting}

Crear script de alerta
\begin{lstlisting}
cat > /root/raidalert
#!/bin/bash
echo $(date) $* >> /root/alert
^D
chmod a+x /root/raidalert
\end{lstlisting}

Monitorear el arreglo con script de alerta
\begin{lstlisting}
mdadm --monitor -1 --scan --config=partitions --program=/root/raidalert
\end{lstlisting}

Crear configuración
\begin{lstlisting}
cat > /etc/mdadm.conf
DEVICE=/dev/sdb1 /dev/sdc1
ARRAY=/dev/md0 devices=/dev/sdb1,/dev/sdc1
PROGRAM=/root/raidalert
\end{lstlisting}

Establecer tarea periódica de monitoreo
\begin{lstlisting}
crontab -e
MAILTO=""
*/2 * * * * /sbin/mdadm --monitor -1 --scan 
\end{lstlisting}

Declarar un fallo
\begin{lstlisting}
mdadm /dev/md0 -f /dev/sdb1
cat /root/alert
\end{lstlisting}

Quitar un disco del array
\begin{lstlisting}
mdadm /dev/md0 -r /dev/sdb1 
cat /root/alert
\end{lstlisting}

Reincorporar el disco al array
\begin{lstlisting}
mdadm /dev/md0 -a /dev/sdb1 
cat /proc/mdstat
cat /root/alert
\end{lstlisting}

Destruir el array
\begin{lstlisting}
mdadm --stop /dev/md0
\end{lstlisting}
